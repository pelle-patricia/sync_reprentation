{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observación de la frecuencia fundamental en el sistema auditivo\n",
    "\n",
    "La detección del pitch es un tema complejo, y elusivo [Licklider,Evans78]. En la comunicación humana (y también entre animales) es una muy antigua via para comunicar mensajes. Existen dificultades tanto en su definición técnica como en su percepción. Un punto notable en ese aspecto es que casi todos los sistemas de reconocimiento de habla de lenguajes no tonales como el inglés por ejemplo, evitan incluir información de pitch en el procesamiento. Desde un punto de vista técnico, la señal de habla es una señal variante en el tiempo, con partes sonoras que podríamos asimilar a conjuntos de senoides armónicamente relacionadas, cuya frecuencia fundamental varía lentamente en el tiempo. La alternancia constante entre segmentos sonoros y no sonoros, la variación temporal de la frecuencia, y además el gran rango posible de frecuencias fundamentales de esas senoides (desde prácticamente 50Hz hasta casi 500Hz) explican en parte esa ausencia en el procesamiento. No es infrecuente que los sistemas de detección de pitch automáticos arrojen resultados sujetos a errores groseros. En las partes sonoras no es inusual obtener valores del doble o de media frecuencia, y en los límites entre sonoro y no/sonoro mediciones completamente alteradas. Cuando la señal además se mezcla con otros estímulos como ruido, la detección suele ser incluso suprimida, por dar resultados no significantes. \n",
    "\n",
    "Desde un punto de vista biológico, es uno de los temas de los que más se ha discutido porque parece ocurrir como resultado de múltiples procesamientos dispersos. Si pensamos a la señal de habla del modo simplista descripto anteriormente, como una suma de tonos individuales armónicos, es natural postular a partir de nuestro conocimiento sobre las particiones cocleares que la mayor información sobre frecuencia fundamental estará en la cóclea en la zona que es exitada por el \"primer armónico\", y en sus correspondientes fibras del nervio auditivo (Helmhotz). Contrariamente a esa suposición, observaciones biológicas desde mediados del siglo XX muestran que hay más información sobre entonación en las zonas de los armónicos alta frecuencia que en los bajos. En los capítulos previos se hizo incapié en la representación sincrónica de las particiones cocleares/fibras del nervio auditivo, basándose en las descripciones individuales de estas particiones. Un aspecto que no se tocó fue que no todas las particiones tienen la misma agudeza de la curva de sintonía, sino que si la partición tiene una frecuencia característica baja su agudeza es mucho mayor que la correspondiente a una frecuencia característica alta. Dicho de una manera simple, las curvas de sintonía tienen un Q proporcional a su frecuencia característica aproximadamente (Shamma [68]). De acuerdo a esta descripción, solo las curvas de sintonía de la parte baja del espectro son capaces de resolver indivudualmente las componentes espectrales de la señal de habla. En la zona de alta frecuencia la situación más común es que más de un armónico impacte dentro del ancho de banda de las curvas de sintonía. Si bien esto parecería indicar que la zona más razonable para encontrar trazas (temporales o frecuenciales) del pitch serían los primeros armónicos, también es necesario recalcar que en la gran mayoría de las señales de habla, el primer armónico suele tener muy baja energía. Incluso algunos medios de comunicación como el teléfono de linea no digital suprimen esa zona de frecuencia. Por otra parte, las mediciones biológicas muestran que para las curvas de sintonía de frecuencia característica de más de 3000Hz, las neuronas muestran disparos de una prominente componente fundamental [delgutte1][]. Se supone que algún mecanismo de combinación de tonos dentro de la partición cochlear/neuronal es capaz de recuperar la fundamental a partir de la presencia de armónicos, aunque su naturaleza es motivo de controversia.\n",
    "\n",
    "Este hecho es en parte el responsable de las discusiones sobre cuál es el mecanismo de detección de frecuencia fundamental (Evans78), dando resultados sorprendentes. Por ejemplo, es posible percibir correctamente la entonación de un sonido que se forma por dos armónicos, cuando ninguno de ellos es de frecuencia fundamental. Pero más interesante aún es que incluso es posible percibir pitch de dos armónicos de este tipo, presentados cada uno en un oído en forma individual, o presentados secuencialmente en lugar de sumados. En el paper fundacional de Goldstein, se propone que el pitch es un fenómeno tan disperso y que obedece a tantas fuentes que necesariamente debería haber un procesamiento central que estadísticamente logre combinar la información distribuida en orden de alinearla en un solo resultado común que describa la entonación. En esta parte final de este capítulo el sistema que presentamos adhiere a esta teoría que consideramos además evolutivamente atractiva, dada la importancia que tuvo la entonación en la comunicación y su antigüedad, muy anterior a la evolución del lenguaje de los humanos. \n",
    "\n",
    "El sistema que proponemos a continuación aplica varios aspectos biológicos descriptos en el trabajo de Goldstein, revisado a su vez por varios autores [duifhuis_jasa82]. El outline del sistema sigue practicamente las mismas premisas mencionadas allí: \n",
    "1. The peripheral ear performs a frequency analysis which reveals what frequency components are present.\n",
    "2. Information on each resolved frequency component fi(i: 1,N) is conveyed stochastically to a \"central processor\". This provides the central processor with a set of independent stochastic representations (described with Gaussian probability density functions) of the component frecuencies. \n",
    "3. The central processor makes an optimum estimate (maximum likelihood estimation) of the unknown stimulus fundamental on the assumption that the stimulus frequencies are unknown harmonics. It turns out that this estimation can be split into two successive steps. The first optimally labels the frequencies with harmonic numbers $z_i$, the second determines the maximum likelihood estimates $F_0$ based on the set of frequencies and their corresponding. \n",
    "\n",
    "En el sistema que proponemos, el análisis frecuencial del punto 1 varía con respecto a la de esos autores ya que se realizará obteniendo la información de la fundamental y sus armónicos a partir de datos tomados de los armónicos superiores, y no de componentes de resolución individual en las partes bajas del espectro. La hipótesis desarrollada fundamentada en las descripciones biológicas, es que en las partes altas del espectro la falta de discriminación individual de las frecuencias dará origen a la información de frecuencia fundamental de alguna manera. Por otra parte respecto a los puntos 2 y 3, el procesamiento estadístico central se realizará en un entorno Bayesiano, y no de máxima Verosimilitud como en la bibliografía anterior. Estas herramientas estadísticas no son nuevas, pero sí su uso despertó interés especialmente en la última década gracias al aumento del poder de cómputo que permite imprementarlas mediante algoritmos intensivos. El entorno Bayesiano combina los dos pasos mencionados en el punto 3 en un solo planteo conjunto del problema, permitiendo que la determinación del número de armónico sea también una variable aleatoria. Si bien, se podría arguir que esto también sería posible utilizando el algoritmo EM para mezclas de gaussianas, la extensión de los métodos Bayesianos no paramétricos, nos permitirán además determinar de una manera automática también el número de armónicos/clases presentes en nuestro espectro. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del sistema propuesto \n",
    "\n",
    "El sistema para recuperar la fundamental a partir de la información de alta frecuencia de la señal de habla, fue propuesto anteriormente en <a href=\"#sabi17\"></a>, y en lo que sigue haremos una revisión del mismo. La descripción es autocontenida, incluso en el desarrollo del sistema estadístico final, si bien esta descripción solo apuntará a entender los principios del entrenamiento bayesiano, sin rigurosidad matemática, para poder desarrollar las ecuaciones de adaptación a nuestro problema de un modo ordenado y conceptual. \n",
    "\n",
    "Como dijimos anteriormente, el sistema tiene una primera etapa de análisis de las frecuencias componentes de la señal de habla, basada en la información de alta frecuencia de la misma. En esta primera etapa un banco de filtros pasabanda descompone la señal en varias señales de diferente distribución de energía en frecuencia. Estas bandas de frecuencia cubren la parte alta del espectro de la señal de habla y se superponen ligeramente en su pendiente inferior. La elección de la respuesta en frecuencia de los filtros pasabanda así como su posición se fundamenta en el estudio de la selectividad y de la relación frequency-place descripta anteriormente en la descripción biológica del sistema auditivo. Los filtros son de $Q$ prácticamente constante y la distribución de frecuencias características están equiespaciados en escala Mel. Esta escala tiene también fundamentación biológica, y ha mostrado dar resultados superiores desde el punto de vista práctico en la representación de las señales de habla (DENG). La siguiente figura muestra el espectro de dos señales correspondientes a una vocal (sintética), la misma en ambos casos pero con diferente frecuencia fundamental, y superpuesto a ellas se muestra también la respuesta en frecuencia de los filtros pasabanda. El gráfico superior podría corresponder a un hombre (barítono posiblemente), mientras que la inferior a un niño o una mujer de voz aguda. Es decir, casi ambos extremos del span de posibles frecuencias fundamentales en los humanos.\n",
    "\n",
    "<img src=\"fig/vocales_con_canales.png\" alt=\"Ejemplo de vocal con filtros de canal\" width=\"300\"/>\n",
    "<a id=\"vocales\"></a>\n",
    "\n",
    "El tipo de respuesta en frecuencia de los filtros pasabanda es el mismo que utilizamos en la representación sincrónica. Cada una de estas señales pasabanda, que de ahora en adelante denominaremos canales, no contienen suficiente energía en el primer armónico, sino solo una mezcla de armónicos superiores. Para generar una frecuencia fundamental a partir de la frecuencias de armónicos superiores al primero, es necesario algún tipo de alinealidad. En nuestro sistema hemos elegido una rectificación a la salida de cada filtro para realizar dicho efecto. Esta elección está fundamentada en la rectificación que produce en el órgano de Corti, adosado a la membrana coclear. Por la disposición mecánica/neuronal, los disparos en las fibras se producen cada vez que la onda realiza un movimiento en una única dirección con respecto a la posición base de la membrana. En la siguiente figura vemos el espectro de una señal que podría representar la salida de uno de los filtros pasabanda, y el espectro correspondiente a la señal rectificada.\n",
    "\n",
    "<img src=\"fig/rectif.png\" alt=\"Rectificación y recuperación de la F0\" width=\"300\"/>\n",
    "<a id=\"rectif\"></a>\n",
    "\n",
    "En la figura superior se muestra el espectro de la señal a rectificar compuesta por dos tonos separados en una frecuencia $\\Delta F$. En el gráfico inferior vemos el espectro de la señal rectificada, donde debido a la rectificación aparecen además de componentes de frecuencia cero, armónicos de frecuencia fundamental $\\Delta F$, como habíamos anticipado. Esta aparición de armónicos de la fundamental puede entenderse si pensamos a la rectificación como la multiplicación en tiempo de la señal original con otra señal de tipo rectangular, que vale uno cuando la señal original es positiva y cero en otro caso. En la figura se muestra en rojo el espectro de esta señal rectificadora de media onda rectangular, cuyos armónicos más importantes ocurren en las mismas frecuencias de la entrada, más otros armónicos inferiores y superiores. La rectificación en el tiempo es la multiplicación de ambas señales, la original y la rectangular, y por lo tanto en el espectro  equivale a una convolución de sus espectros. Este efecto origina el primer armónico de gran amplitud a frecuencia $\\Delta F$, que es donde se produce la mayor energía de la convolución. \n",
    "\n",
    "El diseño del banco de filtros se realiza a partir de las observaciones anteriores: determinada la forma general de los filtros, el parámetro que se debe definir es el ancho de banda, que debe ser tal que permita obtener al menos dos armónicos de la señal de entrada. Si se revisa la explicación anterior sobre la aparición de la fundamental por rectificación, si hubiese un solo armónico en la señal pasabanda, los nuevos armónicos que aparecen por la rectificación están ahora situados a dos veces la frecuencia del armónico, y no en la fundamental, con lo cual no hay un fenómeno de recuperación de $F_0$. Es decir, para recuperar la fundamental por rectificación en la parte alta del espectro es necesario que haya al menos dos armónicos de energía comparable en la señal pasabanda. El factor limitante entonces es que los ancho de banda de los filtros puedan contener al menos 2 armónicos de la mayor frecuencia fundamental esperada, como se puede ver en la primera figura de esta sección, en la parte inferior. La distribución de las frecuencias características de cada filtro se realiza en una escala Mel, como se explicó anteriormente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinación de $F_0$ a partir de los espectros de los canales rectificados. \n",
    "\n",
    "El método que utilizaremos para la lectura de las frecuencias requeridas para el procesamiento central estadístico tiene como entrada las señales pasabandas anteriores, conteniendo información de frecuencia fundamental, pero también de otros armónicos. Las señales de los diferentes canales en cierta forma guardan una relación redundante entre sí, pero serán ligeramente diferentes por provenir de diferentes armónicos superiores, y por cómo estos armónicos fueron filtrados. Siguiendo los lineamientos antes expuestos, el sistema estadístico usará esta redundancia de manera de obtener una lectura directa de la frecuencia fundamental a partir de sus parámetros. Para determinar esas frecuencias, se postula que la señal de cada canal rectificado se aplique como entrada a una combinación de representaciones sincrónicas similar al utilizado en <a href=\"#interspeech06\"></a>. En dicha implementación, el espectro analizado era directamente el de la señal de habla, lo cual presenta una limitación importante cuando el armónico fundamental es muy débil, y los superiores tienen más energía. El espectro de los canales rectificados en cambio siempre presentarán un primer armónico importante, debido a la rectificación, aun cuando la señal original no lo contenga. En la figura siguiente se esquematiza el proceso de obtención del valor de la frecuencia de las principales componentes del espectro:\n",
    "\n",
    "<img src=\"sabi17/figs/diag_bloques1.png\" alt=\"Bloques 1\" width=\"800\"/>\n",
    "<a id=\"diag_bloques1\"></a>\n",
    "\n",
    "En la parte izquierda de esta figura vemos la combinación de representaciones de sincronía. Las respuesta en frecuencia de los filtros pasabanda también tienen una superposición parcial como en los pasabanda descriptos anteriormente, cubriendo entre todos la zona del espectro de la cual se quiere obtener los valores. El objetivo de los filtros es seleccionar el rango de frecuencia a las que cada PLL tiene que sincronizarse. Como en este caso lo que deseamos obtener es que el PLL nos indique la frecuencia de un armónico, los filtros deberían ser suficientemente angostos como para asegurar que haya un solo armónico a la entrada del PLL correspondiente, de modo que la medición de la frecuencia sea como en el caso de las curvas de sintonía. Como vimos en las simulaciones sobre aspectos no lineales biológicos, la presencia de dos armónicos dentro de la banda de paso del filtro producen efectos no lineales que podrían conducir a falta de adquisión (supresión de dos tonos) o al menos una lectura distorsionada de la frecuencia (la frecuencia modulada con dos tonos). De acuerdo a esto, el ancho de banda de los filtros es el factor de diseño clave. En este caso, para que haya mayormente un armónico en cada filtro, el ancho de banda estará definido por la frecuencia fundamental más baja que se supone que se presentará al sistema. El resto de las definiciones sobre el filtro sequirán las mismas reglas que la represetación sincrónica, y la distribución de sus frecuencias características seguirá la distribución lineal en escala mel. \n",
    "\n",
    "En la parte de la derecha de la figura vemos un ejemplo de mediciones obtenidas, para un dado espectro de señal (dibujado en este caso como un espectrograma, dado que los espectros de las señales de habla son en realidad variantes en el tiempo). Sobre el espectrograma se muestra las salidas de frecuencia de los PLLs en línea continua negra. La mayor parte de las frecuencias leídas coinciden con los valores de las frecuencias de los armónicos, pero también es posible ver muchas mediciones de frecuencias espurias, es decir frecuencias que no corresponden a ningún armónico. Esto se debe a que la restricción establecida para el ancho de banda de los filtros puede dar en la mayor parte de los casos que no haya energía de ningún armónico, de modo que alguno de los PLLs no tendrán suficiente energía para producir la adquisición. Por lo tanto es necesario utilizar la correspondiente señal de lockin del PLL para validar los valores de frecuencia obtenidos. Los puntos negros superpuestos indican las mediciones de frecuencias, que se realizarán cada intervalos determinados de tiempo, y que además son consideradas válidas después de chequear el valor correspondiente de lockin. Como se puede ver en la figura, hay líneas de frecuencia en lugares que no tienen mucha energía, pero los puntos (frecuencias validadas como realmente en adquisición) mayormente coinciden con la frecuencia de los armónicos. El modelo estadístico posterior que combinará todas las mediciones de todos los canales rectificados se espera que pueda lidiar con los pequeños errores cometidos en esta selección o con las desviaciones en las mediciones de las frecuencias. \n",
    "\n",
    "Para la obtención de los valores de las frecuencias se usan 60 representaciones sincrónicas por canal, con un rango de frecuencias características entre 50 y 1000Hz, lo que nos permite registrar al menos información de 2 armónicos aún para la frecuencia de entonación más alta. La frecuencia de corte de los filtros también está equiespaciada en escala mel, y también son FIR de orden 2048. Los parámetros de los PLLs fueron seteados experimentalmente en trabajos mencionados anteriormente y su implementación discreta es como se vio en las secciones anteriores (véase \\cite{pc:euro07}). El único cambio en este caso es que los PLL tienen un AGC, Automatic Gain Control, de ganancia más importante que facilite la adquisición, ya que la parte superior del espectro de las señales puede tener energía muy baja. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Obtención de la frecuencia fundamental\n",
    " \n",
    "Los valores obtenidos de frecuencias validadas en las representaciones sincrónicas de todos los canales en cada ventana de tiempo, se considerarán como muestras de una población que obedece a algún modelo estadístico. Dos ejemplos de histograma de estos valores de frecuencias validadas para un cierto instante de tiempo se muestran en la Fig siguiente, para dos señales de habla diferentes. Los histogramas son representados con barras azules, y superpuestos a ellos puede verse un posible modelo probabilístico de distribución de estas frecuencias correspondiente a cada caso (en líneas continuas de colores). \n",
    "\n",
    "<img src=\"sabi17/figs/armonicos.png\" alt=\"Armónicos\" width=\"400\"/>\n",
    "<a id=\"armonicos modelo estadistico\"></a>\n",
    "\n",
    "El modelo estadístico que postulamos en concordancia con lo dicho anteriormente para describir los datos obtenidos es una mezcla de gaussianas, ya que las mediciones pueden tener dispersiones alrededor de las verdaderas frecuencias de cada armónico. Es decir,\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x|\\mu_k,\\lambda_k^{-1})\\label{eq:modelomixgausgeneral}\n",
    "$$\n",
    "\n",
    "donde $x$ es el valor de una posible frecuencia, $\\mathcal{N}(x|\\mu_k,\\lambda_k^{-1})$ es una gaussiana de media $\\mu_k$ y precisión $\\lambda_k = 1/\\sigma_k^2$ asociada a la clase $k$, y $\\pi_k$ el correspondiente peso de la mezcla. A fin de que $p(x)$ corresponda a una función de densidad, se deberá cumplir que la suma de todos los pesos sea uno. Este tipo de modelo podría asimilarse a los modelos utilizados en problemas de clustering en el cual cada observación correspondiente a las salidas los PLL de las representaciones sincrónicas. Las observaciones se aglutinan en torno a los armónicos más energéticos en los espectros de los canales rectificados. De este modo, podemos considerar a las muestras de frecuencia como muestras de entrenamiento para un modelo estadístico, y el objetivo sería determinar todo el conjunto de parámetros $\\{\\mu_k, \\lambda_k, \\,\\,\\mbox{y}\\,\\, \\pi_k, \\forall k\\}$ de esa mezcla, dadas el ensamble de muestras de frecuencia que llamaremos $\\mathcal{D} =\\{x_i, i =1,\\ldots,n\\}$.\n",
    "\n",
    "Dos aspectos deberían ser tenidos en cuenta en la definición del modelo probabilístico esperado que describa las observaciones. Primeramente si observamos los dos gráficos de la Figura 4 puede verse que las variaciones posibles de la frecuencia fundamental son grandes entre señales diferentes y también incluso dentro de la misma señal. Esto implica por otra parte que la cantidad de clases $K$ del modelo no sea un número fijo, ya que si la frecuencia fundamental es baja podría haber hasta 20 armónicos en el rango de frecuencia cubierto por las representaciones sincrónicas (0 a 1000Hz). En cambio si la frecuencia es alta podría haber tan solo 2 en el mismo rango. Es decir que el número $K$ de clases de la mezcla también es un parámetro desconocido. Este problema será atacado en nuestro sistema con métodos de aprendizaje Bayesianos no paramétricos, que proponen que el número de clases de un modelo de suma de gaussianas se autodetermine a partir de los datos. \n",
    "\n",
    "En segundo término el problema planteado, como una mezcla de gaussianas estándard no haría uso de la relación armónica entre las frecuencias. Si las medias de las gaussianas no guardaran una relación armónica entre sí, el único parámetro de interés en nuestro caso sería la estimación de la media del primer armónico, siendo el problema reducido a un simple descarte de todo lo que no se corresponda a este, para luego calcular su media. Sin embargo, gran parte de la información provista en la armonicidad de las frecuencias se perdería, y además muchas veces se podría tomar erróneamente a un conjunto de muestras outsiders de baja frecuencia como si fuera la fundamental. Para atacar este hecho en nuestro sistema se plantea en realidad el siguiente modelo probabilístico de función de densidad, en concordancia con el modelo original de Goldstein:\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x|k\\,F_0,\\lambda^{-1})\\label{eq:modelomixgaustied}\n",
    "$$ \n",
    "donde se postula las medias de las Gaussianas son todas múltiplos del mismo parámetro $F_0$, y además se propone que todas las gaussianas tengan la misma precisión $\\lambda$. De este modo el problema se reduce a estimar los parámetros $F_0$ ,$\\lambda$ y $\\{\\pi_k\\}$, además del número $K$ de mezclas. De este modo el problema de la estimación nos dará el valor buscado que es $F_0$, y además serán obtenidos los otros parámetros como parte necesaria del procedimiento. Pero en este caso, todas las muestras conjuntamente contribuirán a la estimación de $F_0$ y no solo las que aparecen con valores bajos en la medición. La estimación Bayesiana no paramétrica para esta función de densidad fue propuesta también en <a href=\"#sabi17\"></a> y será reproducida aquí en la siguiente sección de una manera resumida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimación de la distribución de probabilidad armónica por métodos Bayesianos no paramétricos.\n",
    "\n",
    "Los métodos Bayesianos no paramétricos son descriptos en la literatura especializada \\cite{bishopbook}, \\cite{orbanz},\\cite{gershmana}. Un breve resumen del método en general puede verse también en <a href=\"#sabi17\"></a>. En esta sección se describirá la adaptación del método a la mezcla con medias armónicas. Primeramente, en esta sección haremos una exposición de las ideas básicas del mismo que nos permitan definir la modificación de la estimación de nuestra densidad. \n",
    "\n",
    "En los métodos Bayesianos de estimación un punto importante a tener en cuenta es que los parámetros del modelo juegan un rol de variables aleatorias. Esta definición de parámetros aleatorios tiene varias interpretaciones, pero en este caso destacaremos el punto de vista de representar *nuestro estado de conocimiento* sobre el valor de esos parámetros. Dentro de esta interpretación, podemos pensar que *a priori* de la observación de muestras de la distribución, nuestro conocimiento sobre los parámetros es general y arbitrario, pero luego de la observación de datos emitidos por la distribución ese conocimiento cambia y se vuelve menos difuso, e idealmente, se podría obtener incluso un valor único o con muy poca incerteza. La idea principal es que la distribución de probabilidad anterior solo es una parte de la distribución completa, es decir, en el entorno Bayesiano la densidad real es \n",
    "\n",
    "$$\n",
    "p(x) = \\int p(x | F_0 ,\\lambda, \\{\\pi_k\\}). p( F_0 ,\\lambda, \\{\\pi_k\\}) d F_0 d \\lambda d\\pi_1 \\ldots d\\pi_K\n",
    "$$ \n",
    "\n",
    "Esta ecuación muestra que en un entorno Bayesiano la función de distribución de la variables $x$ se completa con la distribución de los parámetros, y es en realidad la marginalización de esas variables aleatorias del problema completo. La expresión dentro de la integral es la función de densidad conjunta de la frecuencia $x$ y los parámetros $F_0 ,\\lambda, \\{\\pi_k\\}$, escrita por Bayes como el producto de una función de densidad de $x$ dado conocidos sus parámetros por la función de densidad de probabilidad de los parámetros. La función de densidad de probalidad de la variable $x$ es resultado en este entorno no solo de su densidad de probabilidad en sí, sino también de nuestro desconocimiento de los parámetros expresado en su función de probabilidad. La expresión anterior, corresponde a probabilidad de la frecuencia sin ningún tipo de observación que nos permita entrenarla, o bien *a priori* de cualquier observación. Lo que en realidad queremos estimar es la función de densidad *a posteriori* de dicha distribución, que representaremos del siguiente modo:\n",
    "\n",
    "$$\n",
    "p(x| \\mathcal{D}) = \\int p(x | F_0 ,\\lambda, \\{\\pi_k\\},\\mathcal{D}). p( F_0 ,\\lambda, \\{\\pi_k\\}|\\mathcal{D}) d F_0 d \\lambda d\\pi_1 \\ldots d\\pi_K\n",
    "$$ \n",
    "\n",
    "En estas expresiones $p( F_0 ,\\lambda, \\{\\pi_k\\})$ es la distribución arbitraria y general de nuestros parámetros, o *a priori*, y  $p( F_0 ,\\lambda, \\{\\pi_k\\}|\\mathcal{D})$ será la distribución obtenida a posteriori de la obtención de las muestras. El caso en que esta última tenga una forma tendiente a una delta de dirac alrededor de algún valor, podemos considerar en nuestra interpretación que ese punto representará *el valor* de los parámetros. Si los datos $\\mathcal{D}$ no son suficientes, o realmente los parámetros no son siempre consistentes (asumiendo que la distribución elegida es consistente con los datos), la distribución final tendrá una dispersión más grande, representando un conocimiento incompleto de los parámetros. \n",
    "\n",
    "De acuerdo a esto, vemos que el objetivo que se persigue en el entrenamiento Bayesiano es definir adecuadas funciones a priori para los parámetros, y proponer una manera de estimar la distribución a posteriori de los mismos, y también de la función de densidad. Esa distribución de los parámetros a su vez también será paramétrica, y para distinguirlo de las variables aleatorias del problema original, a los parámetros de estas distribuciones se los denominará *hiperparámetros*. En su forma más simple posible, el aprendizaje Bayesiano permite definir densidades para los parámetros que tengan la misma forma funcional tanto para la función a priori y como para la posteriori. La diferencia entre ambas será que los hiperparámetros de la función a priori son arbitrariamente seteados, de modo que dé una distribución general, mientras que los hiperparámetros de la densidad a posteriori se calcularán a partir de las muestras en el ensamble $\\mathcal{D}$, que conducirá a una focalización del conocimiento obtenido por medio de ellas.  \n",
    "\n",
    "Esta forma de calcular las distribuciones a posteriori es solo posible cuando hay una única gaussiana. En el caso de mezcla de gaussianas la solución no es directa. Esto ocurre también en los aprendizajes de máxima verosimilitud no Bayesianos, donde la solución se plantea en términos de un algoritmo iterativo, conocido como EM (ver Bishop), que permite llegar a una solución para el valor de los parámetros de todas las gaussianas conjuntamente. En el caso de los métodos Bayesianos hay dos tipos de soluciones propuestas: la solución variacional, que propone una aproximación de la densidad que permita un calculo iterativo de las densidades (como en el caso del algoritmo EM)  ver Bishop, y el muestreo de Gibbs de la densidad que nos propone obtener iterativamente muestras de la densidad completa de los parámetros a posteriori. El planteo desarrollado aquí utiliza esta última solución. Es conveniente en este punto hacer énfasis en que este método de muestreo de Gibbs, nos da como resultado una única muestra de una función de densidad. Retomando la interpretación de los método Bayesianos, si esta densidad de parámetros a posteriori que muestreamos se hubiera focalizado de tal manera de concentrar toda la masa de probabilidad muy cerca de un valor único del conjunto de parámetros, entonces, con una alta probabilidad una muestra de esta densidad nos conducirá a valores de los parámetros cercanos a ese valor. De este modo, estamos suponiendo que el resultado del muestreo de Gibbs es un método útil, sobre la hipótesis de que la función de densidad a posteriori está concentrada, es decir que los datos del ensamble que tenemos para entrenar tienen suficiente información para nuestro problema. Si esto no se cumpliera, sería necesario obtener muchas muestras por Gibbs de la función de densidad de los parámetros, y no solo una para poder describir la función de densidad a posteriori de manera acertada. Esto si bien es posible, tiene un costo computacional elevado, ya que el algortimo de Gibbs en sí es de naturaleza secuencial. \n",
    "\n",
    "Anteriormente dijimos que el problema de determinar la cantidad de gaussianas podía hacerse también dentro de un entorno Bayesiano, cuando se utilizan los métodos no paramétricos. El término *no paramétrico* puede conducir a confusiones, ya que estuvimos hablando de parámetros hasta aquí. Es un término tomado del contexto de las teorias de aprendizaje no supervisado y debe entenderse como que los parámetros pueden ser una cantidad no definida incluso infinita. Esto debe ser entendido en un sentido estadístico aproximadamente como que puede haber una cantidad de parámetros similar a la cantidad de muestras de entrenamiento, y no que no hay parámetros. Los procesos de Dirichlet suelen utilizarse en este caso para definir la aparición de clases a medida que van apareciendo datos. Para una descripción conceptual de los procesos de dirichlet véase por ejemplo (). Hablando groseramente, cuando se utiliza esta interpretación de las distribuciones de sumas de funciones de densidad como resultado de un muestreo de un proceso de Dirichlet, las clases van apareciendo en la medida que los datos se van a agregando a la densidad. El método de muestreo de Gibbs puede ser extendido al caso no paramétrico de una manera muy sencilla y conceptual, como veremos al final de la explicación siguiente. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulación de la solución del problema de las mezclas de guassianas armónicas con métodos Bayesianos nos paramétricos\n",
    "\n",
    "Empezaremos describiendo el caso particular del aprendizaje Bayesiano de una densidad de una sola gaussiana, que es la base que nos permitirá entender la solución para mezclas de gaussianas, y finalmente poder explicar el planteo modificado para mezclas de medias armónicas. En el caso del entrenamiento Bayesiano de una gaussiana de media $\\mu$ y varianza $\\sigma^2 = \\lambda^{-1}$, la densidad elegida para los parámetros será una normal-gamma,\n",
    "\n",
    "$$\n",
    "p(\\mu,\\lambda) = NG(\\mu,\\lambda|m_0,\\kappa_0,\\alpha_0,\\beta_0) = \\mathcal{N}(\\mu; m_0,(\\kappa_0 \\lambda)^{-1}) \n",
    " \\mathcal{G}a(\\lambda; \\alpha_0, rate = \\beta_0)\n",
    "$$\n",
    "\n",
    "La función es una multiplicación de una normal que controla la forma de la distribución de la media $\\mu$ de la gaussiana, y una función Gamma que determina la densidad de la presición $\\lambda = 1/\\sigma^2$. Ambas variables no son independientes ya que la dispersión de la media está relacionada con el valor de la presición. Para esta elección, se puede demostrar que la densidad de los parámetros a posteriori de observar $n$ datos $\\mathcal{D}$ es también una Normal-Gamma, dada por \\cite{bishopbook,murphybook}:\n",
    "\n",
    "$$\n",
    "p(\\mu,\\lambda|\\mathcal{D}) = NG(\\mu,\\lambda|m_n,\\kappa_n,\\alpha_n,\\beta_n), \\mbox{ con }$$\n",
    "\n",
    "$$\n",
    "  m_n  =  \\frac{\\kappa_0 m_0+n\\overline{x}}{\\kappa_0+n}, \\kappa_n = \\kappa_0+n, \\alpha_n = \\alpha_0+ \\frac{n}{2} \\mbox{ y }$$\n",
    "\n",
    "$$\n",
    "  \\beta_n  =  \\beta_0+\\frac{1}{2}\\sum_{i+1}^{n}(x_i-\\overline{x})^2+\\frac{\\kappa_0 \\,n(\\overline{x}-m_0)^2}{2( \\kappa_0+n)}$$\n",
    "  \n",
    "donde $\\overline{x}$ es el promedio de los datos observados. Los hiperparámetros de esta distribución $m_n, \\alpha_n, \\beta_n$ y $\\kappa_n$, son \"actualizados\" a partir entonces del valor de muestras obtenidas de entrenamiento, como habíamos mencionado antes. El subíndice $n$ denota justamente que son los hiperparámetros resultantes de haber observado $n$ muestras de la distribución, explicando además el uso del subíndice $0$ para los hiperparámetros de la distribución a priori. La media $m_n$ es el valor esperado de la función de densidad de probabilidad Gaussiana de $\\mu$, y de las fórmulas anteriores, vemos que la dispersión de la media disminuirá si $\\kappa_n \\lambda$ aumenta. Esto significa que a medida que $n$ es mayor, el hiperparámetro $\\kappa_n$ será mayor permitiendo que nuestro \"desconocimiento\" acerca de la media disminuya. \n",
    "\n",
    "Como dijimos anteriormente, si tenemos una mezclas de gaussianas no será posible utilizar esta actualización de parámetros, ya que no se sabe a qué mezcla (en nuestro caso, a qué armónico) corresponde cada una de las muestras de observación que disponemos. En nuestro caso vamos a implementar una resolución del problema por medio del muestreo de Gibbs. La utilización del algoritmo de Gibbs en el caso de mezclas de gaussianas, implicará ampliar las variables aleatorias que intervienen en el modelo para incluir las variables latentes de pertenencia de una muestra a un armónico (mezcla), que es el dato que faltaría para poder utilizar las actualizaciones anteriores. Si llamamos $\\mathcal{Z}~=~\\{z_i,~i~=~1,~\\ldots,~n\\}$ al conjunto de etiquetas que indican el número de mezcla (armónico) al que están asociadas las observaciones $\\mathcal{D}=\\{x_i,i=1,\\ldots,n\\}$, el objetivo es que el algoritmo de Gibbs nos dé un muestreo de la distribución $P(\\mathcal{Z}|\\mathcal{D})$. Es decir, a partir de la distribución de densidad y de la función elegida para los parámetros se podrá calcular cuál es la probabilidad de que cada uno de los datos $x_i$ pertenezca a una determinada clase. Esa función de densidad, tampoco se puede calcular de forma directa, pero mediante este algoritmo seremos capaces de obtener una muestra del valor de los $z_i$ de acuerdo a esa probabilidad. Una vez determinados los valores de estas etiquetas, se podrán utilizar cada muestra en su correspondiente clase para actualizar los parámetros como se explicó antes. \n",
    "\n",
    "El muestreo de Gibbs propone obtener una muestra de las variables en $\\mathcal{Z}=~\\{z_1,~\\ldots,~z_n\\}$, por medio de la emisión secuencial de una muestra de cada una de las $n$ funciones de densidad de probabilidad condicionales de cada variable $z_i$ en función de las demás del conjunto $P(z_i|\\mathcal{Z}_{\\setminus i},\\mathcal{D})$, donde $\\mathcal{Z}_{\\setminus i} = \\{z_j,j=1,\\ldots,n, j\\neq i \\}$. En cada uno de esos muestreos cíclicos se utilizarán como datos los valores de las demás variables, anteriormente muestreadas. El seteo inicial de valores puede ser aleatorio. Se puede demostrar que luego de varios ciclos de muestreo, las muestras individuales obtenidas de las funciones de densidad condicionales, son verdaderas muestras de la función de densidad conjunta de todas ellas. El algoritmo de Gibbs es uno de los tantos Modelos de Cadenas de Markov de Montecarlo (Monte Carlo Markov Chains, MCMC) \\cite{montecarlo}. \n",
    "\n",
    "En el caso de que la mezcla de gaussianas tenga las medias armónicamente relacionadas y suponiendo conocidas las etiquetas $\\mathcal{Z}$ las fórmulas de actualización de los parámetros serán ahora \n",
    "\n",
    "$$\n",
    "m_n =  \\frac{\\kappa_0 m_0+\\sum_{i=1}^{n}z_i x_i}{\\kappa_0+\\sum_{i=1}^{n}z_i^2}, \\kappa_n = \\kappa_0+\\sum_{i=1}^{n}z_i^2, \\alpha_n = \\alpha_0+ \\frac{n}{2},$$\n",
    "$$\n",
    "  \\beta_n  =  \\beta_0+\\frac{1}{2}\\sum_{i=1}^{n}(x_i-z_i\\overline{x})^2+\\frac{\\kappa_0 \\,\\left(\\sum_{i=1}^{n}z_i\\right)(\\overline{x}-m_0)^2}{2 \\kappa_n}\\label{eq:nuevasact}$$\n",
    "\n",
    "donde el valor estimado de la frecuencia fundamental $F_0$ tendrá una dispersión centrada alrededor de $m_n$, ya  que es el valor de la media para $k=1$.  \n",
    "\n",
    "##### Dem:\n",
    "Para demostrar que la función de probabilidad de los parámetros $F_0$ y $\\lambda$ a posteriori de la observación de los datos $\\mathcal{D}$, con sus etiquetas, tiene la forma expresada en equación anterior, planteamos la probabilidad conjunta de los datos de entrenamiento $\\mathcal{D}$ y los parámetros $F_0$ y $\\lambda$, dadas las variables de las etiquetas $\\mathcal{Z}$, que por Bayes podemos decir que será \n",
    "\n",
    "$$  p(\\mathcal{D},F_0,\\lambda|\\mathcal{Z}) =  p(\\mathcal{D}|F_0,\\lambda,\\mathcal{Z})\\, p(F_0,\\lambda|\\mathcal{Z})$$\n",
    "\n",
    "$$ = \\prod_{i=1}^{n} p(x_i|F_0,\\lambda,\\mathcal{Z})\\, p(F_0,\\lambda)$$\n",
    "\n",
    "$$ = \\prod_{i=1}^{n} \\mathcal{N}(x_i;z_i F_0, \\lambda^{-1}) \\,p(F_0,\\lambda)$$\n",
    "\n",
    "donde vemos que el primer factor es la verosimilitud de los datos, y el segundo factor es la probabilidad a priori de los parámetros, que no depende de $\\mathcal{Z}$ si no depende de $\\mathcal{D}$. Por otro lado, también podemos ver que \n",
    "$$   p(\\mathcal{D},F_0,\\lambda|\\mathcal{Z}) =  p(F_0,\\lambda|\\mathcal{D},\\mathcal{Z})\\, p(\\mathcal{D}|\\mathcal{Z})$$\n",
    "\n",
    "donde el primer factor del lado derecho de la igualdad es la probabilidad que buscamos, es decir la probabilidad de los parámetros a posteriori de los datos y las etiquetas, mientras que el segundo factor no depende de los parámetros. Por lo tanto, igualando (\\ref{eq:priorxlik}) y (\\ref{eq:posteriorxPdeD}), tendremos la relación entre la probabilidad a posteriori y la probabilidad a priori de los parámetros, expresada como\n",
    "\n",
    "$$\n",
    "p(F_0,\\lambda|\\mathcal{D},\\mathcal{Z})\\, p(\\mathcal{D}|\\mathcal{Z}) = \\prod_{i=1}^{n} \\mathcal{N}(x_i;z_i F_0, \\lambda^{-1}) \\,p(F_0,\\lambda)$$\n",
    "\n",
    "También aquí postulamos que la forma funcional de la distribución de los parámetros será una Normal-Gamma:\n",
    "\n",
    "$$\n",
    "NG(F_0,\\lambda |m,\\kappa,\\alpha,\\beta) = \\mathcal{N}(F_0;m,(\\kappa\\,\\lambda)^{-1})\\,\\mathcal{G}a (\\lambda| \\alpha, \\mbox{rate=}\\beta)\n",
    "$$\n",
    "$$ = \\frac{1}{Z_{NG}}\\lambda^{(\\alpha-\\frac{1}{2})}\\, e^{-[\\beta \\lambda + \\frac{\\kappa \\lambda}{2}(F_0-m)^2]}$$\n",
    "$\\mbox{con } Z_{NG}(\\alpha.\\beta,\\kappa) = \\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}\\left(\\frac{2\\pi}{\\kappa}\\right)^{\\frac{1}{2}}\n",
    "$\n",
    "\n",
    "Escribiendo el logaritmo del lado izquierdo de la igualdad en la Ec.(\\ref{eq:conjnueva}) tendremos \n",
    "\n",
    "$$\\log(p(F_0,\\lambda|\\mathcal{D},\\mathcal{Z})\\, p(\\mathcal{D}|\\mathcal{Z}) ) = -\\log(Z_{NG}(\\alpha_n.\\beta_n,\\kappa_n))+\\left(\\alpha_n-\\frac{1}{2}\\right)\\,\\log(\\lambda)- [\\beta_n \\lambda + \\frac{\\kappa_n \\lambda}{2}(F_0-m_n)^2] + \\log(p(\\mathcal{D}|\\mathcal{Z}))$$\n",
    "\n",
    "mientras que el logaritmo del lado derecho será\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n\\log(\\mathcal{N}(x_i;z_i F_0,\\lambda^{-1}) + \\log(p(F_0,\\lambda)) = \\frac{n}{2}\\log(\\lambda) -\\frac{n}{2}\\log(2\\pi) -\\frac{\\lambda}{2}\\sum_{i=1}^n(x_i - z_iF_0)^2 -\\log(Z_{NG}(\\alpha_0.\\beta_0,\\kappa_0))+\\left(\\alpha_0-\\frac{1}{2}\\right)\\,\\log(\\lambda) - [\\beta_0 \\lambda + \\frac{\\kappa_0 \\lambda}{2}(F_0-m_0)^2]$$\n",
    "\n",
    "De igualar ambas expresiones obtendremos por un lado:\n",
    "$$\n",
    "\\log(Z_{NG}(\\alpha_n.\\beta_n,\\kappa_n)) = \\log(p(\\mathcal{D}|\\mathcal{Z}))+ \\log(Z_{NG}(\\alpha_0.\\beta_0,\\kappa_0))+\\frac{n}{2}\\log(2\\pi)$$\n",
    "\n",
    "que es la parte que no contiene a los parámetros $F_0$ y $\\lambda$, y que nos permitiría expresar $p(\\mathcal{D}|\\mathcal{Z}) = Z_{NG}(\\alpha_n.\\beta_n,\\kappa_n)/(Z_{NG}(\\alpha_0.\\beta_0,\\kappa_0) (2\\pi)^\\frac{n}{2})$, mientras que\n",
    "\n",
    "$$\n",
    "\\left(\\alpha_n-\\frac{1}{2}\\right)\\,\\log(\\lambda) = \\left(\\alpha_0-\\frac{1}{2}\\right)\\,\\log(\\lambda)+\\frac{n}{2}\\log(\\lambda)\\nonumber\\Rightarrow$$\n",
    "$$\n",
    "\\alpha_n = \\alpha_0+\\frac{n}{2}\n",
    "$$\n",
    "\n",
    "de donde se obtiene el valor de la actualización de $\\alpha_n$. Por último, es necesario igualar los términos que contienen $F_0$ y $\\lambda$:\n",
    "\n",
    "$$\\beta_n \\lambda + \\frac{\\kappa_n \\lambda}{2}(F_0-m_n)^2 = \\frac{\\lambda}{2}\\sum_{i=1}^n(x_i - z_i F_0)^2 + \\beta_0 \\lambda + \\frac{\\kappa_0 \\lambda}{2}(F_0-m_0)^2$$\n",
    "$$ = \\frac{\\lambda}{2}\\sum_{i=1}^n(x_i^2 - 2 z_i x_i F_0 + z_i^2 F_0^2) + \\beta_0 \\lambda + \\frac{\\kappa_0 \\lambda}{2}(F_0^2 -2 F_0 m_0 + m_0^2) $$\n",
    "\n",
    "Los términos del lado derecho que contengan $F_0.\\lambda$ y $F_0^2 \\lambda$ serán las que formarán $\\frac{\\kappa_n \\lambda}{2}(F_0-m_n)^2$, mientras que los términos que contengan $\\mbox{cte}.\\lambda$ se agruparán para formar  $\\beta_n \\lambda$, de modo que, tomando los términos que contienen $F_0$, tendremos\n",
    "\n",
    "$$\\frac{\\lambda}{2}\\sum_{i=1}^n( z_i^2 F_0^2 - 2 z_i x_i F_0 ) + \\frac{\\kappa_0 \\lambda}{2}(F_0^2 -2 F_0 m_0)$$\n",
    "$$=\\frac{\\lambda}{2}\\left[-2\\left(\\kappa_0 m_0 + \\sum_{i=1}^n( z_i x_i)\\right)F_0 + \\left(\\sum_{i=1}^n z_i^2 + \\kappa_0\\right) F_0^2\\right]$$\n",
    "$$= \\frac{\\kappa_n \\lambda}{2}\\left[-2 m_n F_0 + F_0^2\\right] + \\frac{\\kappa_n \\lambda}{2}m_n^2 - \\frac{\\kappa_n \\lambda}{2}m_n^2$$\n",
    "\n",
    "donde se ha definido $\\kappa_n = \\left(\\sum_{i=1}^n z_i^2 + \\kappa_0\\right)$  y $ m_n = \\frac{\\kappa_0 m_0 + \\sum_{i=1}^n( z_i x_i)}{\\kappa_n}$, que son las fórmulas de actualización. Y por otro lado si se agrupan los términos que contengan $\\mbox{cte}.\\lambda$ para formar  $\\beta_n \\lambda$, tendremos. \n",
    "\n",
    "$$\n",
    "\\beta_n \\lambda  = \\left(\\frac{1}{2}\\sum_{i=1}^nx_i^2 + \\beta_0 + \\frac{\\kappa_0 }{2} m_0^2- \\frac{\\kappa_n }{2} m_n^2\\right)\\lambda \n",
    "$$\n",
    "\n",
    "que finalmente, reordenando los términos, obtendremos\n",
    "\n",
    "$$\n",
    "\\beta_n =  \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n(x_i - z_i \\overline{x})^2 + \\frac{\\kappa_0 \\sum_{i=1}^n z_i}{\\kappa_n}\\left(\\overline{x} - m_0\\right)^2\n",
    "$$\n",
    "\n",
    "que completaría la actualización.\n",
    "\n",
    "#### Algoritmo de Gibbs en el entrenamiento no paramétrico\n",
    "\n",
    "Para lograr muestras de $P(\\mathcal{Z}|\\mathcal{D})$ mediante el algoritmo de Gibbs, faltaría encontrar las funciones condicionales $P(z_i|\\mathcal{Z}_{\\setminus i},\\mathcal{D},a,\\theta)$, donde se han puesto en evidencia los hiperparámetros $a$, que controlan la forma de las variables $z$ y $\\theta=(m,\\kappa,\\alpha,\\beta)$, que son los hiperparámetros de las Normal-Gamma. Para encontrar estas probabilidades discretas, se calculará cada una de las probabilidades de que el valor de $z_i$ sea un valor $k$, $P(z_i = k)$ definido entre\n",
    "los posibles valores de $k$, que son todos las posibles clases/armónicos a la que la muestra puede pertenecer. Utilizando bayes, (salteando algunos pasos intermedios, para más detalles ver \\cite{murphybook}), se tiene que \n",
    "\n",
    "$$\n",
    "P(z_i=k|\\mathcal{Z}_{\\setminus i},\\mathcal{D},a,\\theta) = \\frac{P(z_i=k,\\mathcal{Z}_{\\setminus i},\\mathcal{D}|a,\\theta)}{p(\\mathcal{D},\\mathcal{Z}_{\\setminus i}|a,\\theta)}\n",
    "  = \\frac{P(z_i=k|\\mathcal{Z}_{\\setminus i},a,\\theta) \\,p(\\mathcal{D}|z_i=k,\\mathcal{Z}_{\\setminus i},a,\\theta)}{p(\\mathcal{D}|\\mathcal{Z}_{\\setminus i},a,\\theta)}$$\n",
    "  \n",
    "$$\n",
    "  = \\frac{P(z_i=k|\\mathcal{Z}_{\\setminus i},a) \\,p(x_i|z_i=k,\\mathcal{D}_{\\setminus i},\\mathcal{Z}_{\\setminus i},\\theta)}{p(x_i|\\mathcal{D}_{\\setminus i},\\mathcal{Z}_{\\setminus i},\\theta)}\n",
    "$$\n",
    "\n",
    "Esta probabilidad es entonces el producto de dos factores (salvo normalización): por un lado $P(z_i=k|\\mathcal{Z}_{\\setminus i},a)$, y por otro lado $p(x_i|z_i=k,\\mathcal{D}_{\\setminus i},\\mathcal{Z}_{\\setminus i},\\theta)$. El primer factor corresponde a la estimación de una distribución multinomial para la variable $z_i$ a partir de las muestras en $\\mathcal{Z}_{\\setminus i}$. En la estimación Bayesiana los parámetros de dicha multinomial, que son justamente las probabilidades de que la variable sea igual a $k$, pueden describirse mediante una distribución de Dirichlet, cuya forma a posteriori de la aparición de las muestras será \\cite{murphybook}:\n",
    "\n",
    "$$\n",
    "P(z_i=k|\\mathcal{Z}_{\\setminus i},a) = \\frac{n_{k\\setminus i}+a/K}{n+a-1}\n",
    "$$\n",
    "\n",
    "donde $n_{k\\setminus i}$ es la cantidad de observaciones $x_i$ que fueron etiquetadas como mezcla $k$ (excepto obviamente la muestra $x_i$). El segundo factor es la verosimilitud predictiva de la observación $x_i$ de haber sido generada por la mezcla $k$. Aquí, si el modelo de probabilidad es de mezclas con medias enlazadas como en (\\ref{eq:modelomixgaustied}), se puede demostrar que será una t-Student del tipo \n",
    "$$\n",
    "  p(x|\\mathcal{D}_k) \n",
    "  = t_{(2\\alpha_{n_k})}(x|\\mu_{n_k},\\psi_{n_k}^{-1}), \\mbox{ con } \\psi_{n_k} = \\frac{\\alpha_{n_k} \\kappa_{n_k}}{\\beta_{n_k}(\\kappa_{n_k}+1)}\\nonumber\n",
    "$$\n",
    "\n",
    "Una vez definida la forma de las funciones condicionales (que son multinomiales), se puede realizar el algoritmo de Gibbs. De acuerdo a un valor inicial para todas las muestras que puede ser aleatorio inicialmente, se muestrea cada una de las variables en orden, y cada vez que se muestrea una variable se cambia en el conjunto de muestras su viejo valor por el nuevo, que es tenido en cuenta así para el muestreo de la siguiente variable. Las muestras de cada función son muestras de una función multinomial como vimos antes, que tiene dos partes. Antes de calcular la parte correspondiente a la probabilidad predictiva, deben calcularse los hiperparámetros correspondientes al conjunto $\\mathcal{Z}_{\\setminus i}$ y luego evaluarse en $x_i$. Luego de unas cuantos ciclos de muestreo de todos los $z_i$, se puede concluir que la secuencia de muestreos será una muestra de la función conjunta $P(\\mathcal{Z}|\\mathcal{D})$. En ese momento con los hiperparámetros actualizados de acuerdo a las etiquetas obtenidas, es posible conocer la densidad de $F_0, \\lambda$, y $\\boldsymbol{\\pi} = \\{\\pi_k, k =1,\\ldots,K\\}$, que constituirá una muestra de la familia de funciones de densidad de los parámetros. Nuevamente, si la distribución $(\\mathcal{Z}|\\mathcal{D})$ es concentrada, la muestra obtenida de las variables será bastante definida, y por lo tanto la familia de funciones de los parámetros no será muy amplia, idealmente, una sola función. Esto se puede verificar experimentalmente haciendo varios muestreos de Gibbs, empezando de etiquetas aleatorias, y si siempre se obtiene el mismo valor resultante para cada variable $z_i$ para todas las variables, será una indicación de que la distribución de etiquetas es única, y la función de densidad de los parámetros también.\n",
    "\n",
    "La extensión no paramétrica permite para el caso de mezclas de gaussianas plantear que la distribución  a priori de los parámetros de las etiquetas $P(\\boldsymbol{\\pi})$ no tiene un número limitado $K$. La interpretación de los procesos de Dirichlet mediante la ilustración de restaurant chino, nos permite adaptar el procedimiento anterior de la siguiente manera simple:  \\cite{yee},\\cite{ferguson} en lugar de estimar los parámetros de la distribución multinomial, los valores de $P(z_i = k|\\mathcal{Z}_{\\setminus i},a)$ será ahora:\n",
    "\n",
    "$$\n",
    " p(z_i=k|\\mathcal{Z}_{\\setminus i},a)= \\left\\{ \\begin{array}{rcl}\n",
    "\t\t\t\t       \\frac{n_{k\\setminus i}}{n+a-1}& \\mbox{si}&\n",
    "\t\t\t     n_{k\\setminus i}>0\\\\\n",
    "\t\t\t     \\frac{a}{n+a-1}& \\mbox{si}& k=K+1\\\\\n",
    "\t\t\t\t   \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Es decir, según el último renglón de la fórmula existe una probabilidad proporcional a $a$ de que una nueva clase sea creada. También la expresión del caso en que sí hay muestras de la clase es diferente: aquí se contempla la posibilidad de que durante el muestreo cíclico de las variables de las etiquetas una clase desaparezda: si una clase $k$ tenía 1 muestra ($n_{k\\setminus i} = 1 > 0$) y la etiqueta de esa muestra cambia después de muestrearla, la probabilidad de esa clase será cero, y la clase \"desaparece\". De este modo, la cantidad de armónicos será \"la que necesiten los datos\". El parámetro de control del crecimiento de clases, es el parámetro $a$, que puede interpretarse como una cantidad de muestras \"ficticias\" que están en una clase no observada hasta el momento, y que tiene una cierta chance de ocurrir. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados obtenidos\n",
    "El desempeño es evaluado usando dos bases de datos de habla con información de referencia de la frecuencia fundamental, disponibles libremente. La referencia es obtenida mediante una grabación complementaria simultánea de señal laringográfica. Dicha señal se considera que contiene practicamente solo información del primer armónico, que dentro del ámbito de aplicaciones es lo que consideraremos que se corresponde al pitch de la emisión. La primera se denomina *Keele pitch extraction reference database  \\cite{Keeledb}*, y contiene señales de habla de 5 hombres y 5 mujeres que leen una historia corta, de alrededor de 35 segundos de duración. La segunda, producida por P. Bagshaw \\cite{Bagshaw}, está compuesta de 50 frases emitidas cada una de ellas por un hablante hombre y uno mujer, el mismo hablante para las 50 frases,  produciendo una duración total de señales de unos 7 minutos. Ambas bases de datos son calidad estudio, muestreadas a 20KHz. La referencia en ambas bases de datos es calculada por los autores utilizando la señal laringográfica, que es evaluada cada 10mseg. Se realizan los experimentos de medición de frecuencia de entonación para las condiciones originales de las señales y también para ruido sumado en varios niveles de relación de señal a ruido (Signal to Noise Ratio, SNR). Para los experimentos con ruido se adicionan señales ejemplo de la base de datos NOISEX \\footnote{\\texttt{<http://spib.linse.ufsc.br/noise.html>}}. Los resultados obtenidos con el sistema propuesto son comparados con los obtenidos con el algoritmo RAPT~\\cite{talkin}, que es uno de los algoritmos más conocidos para extracción de frecuencia de entonación. Los resultados para este algoritmo son generados utilizado el toolkit \\textbf{Wavesurfer}\\footnote{\\texttt{<http://www.speech.kth.se/wavesurfer/>}}, también disponible libremente. Todos los parámetros del algoritmo son utilizados en sus valores por defecto, salvo los rangos de frecuencia de entonación seteados entre 50 to 500Hz. \n",
    "\n",
    "Las mediciones de error son evaluadas en términos del porcentaje de error grosero (gross error rate, GER), el cual es medido como el porcentaje de valores de frecuencia estimada que tienen un error absoluto mayor que un cierto porcentaje (20\\% en este caso) con respecto a la referencia. Los errores del algoritmo de referencia pueden ser de dos tipos: por un lado un error debido una estimación errónea, y por el otro lado que el algoritmo interprete que ciertas muestras de la señal sonora son sordas, en cuyo caso da frecuencia cero. Como no es posible deshabilitar la detección sonoro/sordo para el algoritmo RAPT, tomaremos las mediciones del error grosero sobre dos conjuntos diferentes de muestras como en \\cite{Wang}: se miden porcentaje de errores groseros sobre el Total de muestras sonoras (Total), y se medirán también los errores restringido al subconjunto de las muestras sonoras de señal donde RAPT da una estimación distinta de cero (Restringido).\n",
    "\n",
    "Las tablas \\ref{table:1} y \\ref{table:2} muestran los porcentajes de errores groseros para el sistema presentado y para RAPT, para varios niveles de ruido blanco, desde limpio a 0dB de relación señal a ruido. \n",
    "\n",
    "\n",
    "<img src=\"fig/bagshaw.png\" alt=\"Tabla I resultados Bagshaw\" width=\"500\"/>\n",
    "<a id=\"chev_resultados\"></a>\n",
    "<img src=\"fig/Keele.png\" alt=\"Tabla II resultados Keele\" width=\"500\"/>\n",
    "<a id=\"Keele_resultados\"></a>\n",
    "\n",
    "\n",
    "De los resultados obtenidos primeramente podemos ver que mediante el diseño que se propuso es posible sin dudas obtener medición de frecuencia de entonación a partir de los armónicos de alta frecuencia, ya que estos resultados presentan poco error comparados a la referencia laringográfica, como se puede observar en las mediciones para habla limpia. Por otra parte la ventaja de esta implementación basado en representaciones sincrónicas desde el punto de vista práctico es evidente a medida que el ruido es mayor. Si observamos los resultados en señal limpia, los errores son mejores que el algoritmo RAPT sobre el total de las muestras sonoras, y comparables dentro de la zona donde RAPT da medición, siendo un poco mejor los resultados de la base de datos Bagshaw que en Keele. Pero a medida que aumentamos el nivel de ruido en la señal, se puede observar que los errores en el total de la señal sonora no se modifica prácticamente, variando entre 1 y 2\\% con respecto a señal limpia hasta ruidos de una intensidad de hasta 10 dB de SNR, y sólo aumenta al orden del 12-15\\% para igual ruido que señal (0dB de SNR). Bajo estas condiciones, el algoritmo RAPT deja directamente de ser conveniente, ya que los errores nos muestran que más de la mitad de las muestras sonoras son incorrectamente detectadas en ambas bases de datos. Este efecto muestra una ventaja intrínseca de la sincronía para la medición de las frecuencias, ventaja derivada de la robustez de los PLLs en entornos de ruido.\n",
    "\n",
    "Por último debemos mencionar la ventaja que representa este esquema central de procesamiento de múltiples fuentes de información. En nuestro caso toda la información provenía de espectros similares, pero todos están basados en la misma suposición de reconstrucción de la fundamental por rectificación de múltiples armónicos. Si se tienen mestras de la frecuencia de otras fuentes diferentes, la ampliación para esas muestras es inmediata, permitiendo eventuales mejoras de los resultados prácticos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
